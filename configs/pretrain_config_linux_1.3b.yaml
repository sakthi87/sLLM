# Pretraining Configuration - 1.3B Parameter Model
# Optimized for RedHat Linux (128GB RAM, 24 CPU cores, 1TB SSD)
# Target: Better quality + more natural English + stable JSON generation

model:
  # 1.3B parameter architecture (1.211B parameters)
  d_model: 1536             # Hidden dimension (increased from 1280)
  n_layers: 40              # Number of transformer layers (increased from 36)
  n_heads: 24               # Number of attention heads (d_model / 64)
  d_ff: 6144                # Feed-forward dimension (4 * d_model)
  max_seq_len: 2048         # Maximum sequence length (increased from 1024 for better coherence and JSON handling)
  dropout: 0.15             # Dropout rate (maintained for regularization)

training:
  # Training parameters - OPTIMIZED for 128GB RAM and 24 CPU cores
  # Memory analysis: With batch_size=20, uses ~25-30GB (20-23% of 128GB)
  # Optimal batch size for 1.3B model - still 77-80% RAM headroom available!
  batch_size: 20            # Batch size (reduced from 24 for 1.3B model)
  gradient_accumulation_steps: 8  # Effective batch size = 160 (20 * 8)
  num_epochs: 100           # Phase 1: General English pretraining
  learning_rate: 3e-4       # Learning rate (standard for GPT-style models)
  weight_decay: 0.02        # Weight decay for regularization
  num_workers: 24           # Use all 24 CPU cores for data loading
  pin_memory: true          # Pin memory for faster GPU transfer (if CUDA available)
  save_every: 5             # Save checkpoint every 5 epochs
  output_dir: "./models/pretrained_1.3b"
  
  # Training improvements
  use_mixed_precision: true # Enable mixed precision (FP16) for faster training
  max_grad_norm: 1.0        # Gradient clipping to prevent instability
  
  # Learning rate schedule
  use_warmup: true          # Enable learning rate warmup
  warmup_steps: 1000        # Warmup steps (recommended: 500-1000)
  use_cosine_decay: true    # Use cosine annealing decay
  cosine_restart_epochs: 10 # Restart cosine schedule every 10 epochs
  label_smoothing: 0.1      # Label smoothing for better generalization
  
  # Validation and monitoring
  val_split: 0.1            # Validation split (10% of data)
  early_stopping: true      # Enable early stopping
  early_stopping_patience: 5  # Patience for early stopping (epochs)
  target_loss: 2.0          # Target loss for Phase 1 (slightly lower for better quality)
  use_tensorboard: true     # Enable TensorBoard logging
  tensorboard_log_dir: "./runs/pretraining_1.3b"
  
  # Checkpoint cleanup - Disk space management
  cleanup_checkpoints: true  # Automatically clean up old checkpoints
  keep_last_checkpoints: 3  # Keep last N checkpoints (1.3B checkpoints are ~4-5GB each)
  keep_milestone_checkpoints: true  # Keep checkpoints at milestones (every 10th epoch)
  milestone_interval: 10    # Milestone interval (e.g., 10 = every 10th epoch)
  keep_best_checkpoint: true  # Keep checkpoint with lowest loss

data:
  # Phase 1: General English pretraining data
  # Use large English corpus (Wikipedia, books, web text)
  pretraining_data_path: null  # null = use all data directories
  data_dir: "./data"           # Base directory for all data
  max_samples: null            # Limit number of samples (null = use all)
  
  # Data sources (will be loaded from data/ directory):
  # - data/pretraining/ - General English text
  # - data/conversational/ - Conversational datasets (for fluency)
  # Note: Domain-specific data will be used in Phase 2 (fine-tuning)

tokenizer_path: "./tokenizer"  # Path to trained tokenizer
# IMPORTANT: For GPT-2 Large/1.3B, tokenizer should have vocab_size=50257
# Train with: python scripts/train_tokenizer.py --vocab_size 50257

